<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>

<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Insert Sorts</title>
<link rel=STYLESHEET href="/~rhowell/style.css">

</head>

<body>
<div class="NAVBAR">
<a href="/~rhowell/DataStructures/redirect/merge-sorts">Next: Merge
      Sorts 
      </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
      href="/~rhowell/DataStructures/redirect/select-sorts">Previous:
      Select Sorts
      </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
      href="/~rhowell/DataStructures/redirect/sorting">Up: Sorting
      </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
      href="/~rhowell/DataStructures/">Top</a>
</div>

<h1>Insert Sorts</h1>

An insert sort operates by repeatedly inserting an element into a
    sorted portion of the array.  Thus, as for select sorts, at each
    step the data items will be arranged into a sorted part, followed
    by an unsorted part; however, for insert sorts, there is no
    restriction on how elements in the unsorted part compare to
    elements in the sorted part.  The following figure illustrates
    this arrangement.
<p>
<img src="insert-sorts.jpg"
    alt="The arrangement at each step of an insert sort.">
</p>
Initially, the sorted part will contain the first element, as a single
    element is always sorted.  At
    each step, the first element in the unsorted part is inserted into
    its proper location in the sorted part.  As a result, the sorted
    part now contains one more element, and the unsorted part one
    fewer element.  After <i>n</i>&nbsp;-&nbsp;1 steps, where <i>n</i>
    is the number of elements in the array, the sorted part will
    contain all the elements, and the algorithm will be done.
<p>
Again, this approach can be implemented in various ways.  The main
      difference in these implementations is in how we insert an
      element.  The most straightforward way is as follows:
</p>
<ol>
<li> Copy the first element of the unsorted part to a temporary
	variable.</li> 

<li> Iterate from the location of the first
	element of the unsorted part toward the front of the array
	as long as we are at an index greater than 0 and the
	element to the 
	left of the current index is greater than the
	element in the temporary variable.  On each iteration:
<ul>
<li> Copy the element to the left of the current index to the
	    current index.
</ul>
</li>
<li> Place the value in the temporary variable into the location at
	which the above loop stopped.</li> 
</ol>
The algorithm that uses the above insertion technique is known as
    <i>insertion sort</i>.  Like selection sort, it requires an outer
    loop to keep track of the number of elements in the sorted part.
    Each iteration of this outer loop performs the above insertion
    algorithm.  It is not hard to see that this algorithm is <a
    href="/~rhowell/DataStructures/redirect/stable-sort">stable</a>. 
<p>
The main advantage insertion sort has over <a
      href="/~rhowell/DataStructures/redirect/select-sorts">selection
      sort</a> is that the inner loop only iterates as long as
      necessary to find the insertion point.  In the worst case, it
      will iterate over the entire sorted part.  In this case, the
      number of iterations is the same as for selection sort; hence,
      the worst-case running time is in <i>O</i>(<i>n</i><sup>2</sup>)
      - the same as selection sort and <a
      href="/~rhowell/DataStructures/redirect/bubble-sort">bubble
      sort</a>.  At the other extreme, however, if the array is
      already sorted, the inner loop won't need to iterate at all.  In
      this case, the running time is in <i>O</i>(<i>n</i>), which is
      the same as the running time of bubble sort on an array that is
      already sorted.
</p>
Unlike bubble sort, however, insertion sort has a clean
    characterization of its performance based on how sorted the array
    is.  This characterization is based on the notion of an
    <i>inversion</i>, which is a pair of array locations
    <i>i</i>&nbsp;&lt;&nbsp;<i>j</i> such that the value at location
    <i>i</i> is greater than the value at location <i>j</i>; i.e.,
    these two values are out of order with respect to each other.  A
    sorted array has no inversions, whereas in an array in reverse
    order, every pair of locations is an inversion, for a total of
    <i>n</i>(<i>n</i>&nbsp;-&nbsp;1)/2 inversions.  In general, we can
    say that the fewer inversions an array has, the more sorted it is.
<p>
The reason why inversions are important to understanding the
      performance of insertion sort is that each iteration of the
      inner loop (i.e., step 2 of the insertion algorithm above)
      removes exactly one inversion.  Consequently, if an array
      initially has <i>k</i> inversions, the inner loop will iterate a
      total of <i>k</i> times.  If we combine this with the
      <i>n</i>&nbsp;-&nbsp;1 iterations of the outer loop, we can
      conclude that the running time of insertion sort is in
      <i>O</i>(<i>n</i>&nbsp;+&nbsp;<i>k</i>).  Thus, if the number of
      inversions is relatively small in comparison to <i>n</i> (i.e.,
      the array is nearly sorted), insertion sort runs in
      <i>O</i>(<i>n</i>) time.  (By contrast, <i>n</i>&nbsp;-&nbsp;2
      inversions can be enough to cause the inner loop of bubble sort
      to iterate its worst-case number of times.)  For this reason,
      insertion sort is the algorithm of choice when we expect the
      data to be nearly sorted - a scenario that occurs frequently in
      practice.  This fact is exploited by an efficient hybrid
      algorithm that combines insertion sort with two other sorting
      algorithms - see &quot;<a
      href="/~rhowell/DataStructures/redirect/hybrid-sorts">Hybrid
      Sorts</a>&quot; for more details. 
</p>
Before we consider another insert sort, there is one other advantage
    to insertion sort that we need to consider.  Because the algorithm
    is simple (like selection sort and bubble sort), it performs well
    on small arrays.  More complex algorithms like <a
    href="/~rhowell/DataStructures/redirect/heap-sort">heap sort</a>,
    while providing much better worst-case performance for larger
    arrays, don't tend to perform as well on small arrays.  In many
    cases, the performance difference on small arrays isn't enough to
    matter, as pretty much any algorithm will perform reasonably well
    on a small array.  However, this performance difference can become
    significant if we need to sort many small arrays (in a later
    section, we will see an application in which this scenario
    occurs).  Because insertion sort tends to out-perform both
    selection sort and bubble sort, it is usually the best choice when
    sorting small arrays.
<p>
<a name="tree-sort"></a>
Another way to implement an insert sort is to use a balanced binary
      search tree, such as an <a
      href="/~rhowell/DataStructures/redirect/avl-trees">AVL tree</a>,
      to store the sorted part.  In order to do this, we need to
      modify <a
      href="/~rhowell/DataStructures/redirect/bst-intro">the
      definition of a binary search tree</a> to allow multiple
      instances of the same key.  In order to achieve stability, if we
      are inserting a key that is equal to a key already in the tree,
      we would treat the new key as being greater than the
      pre-existing key - i.e., we would recursively insert it into the
      right child.  Once all the data items are inserted, we would
      then copy them back into the array in sorted order using an <a
      href="/~rhowell/DataStructures/redirect/inorder-traversal">inorder
      traversal</a>.  We call this algorithm <i>tree sort</i>.
</p>
This algorithm doesn't exactly match the above description of an
    insert sort, but it is not hard to see that it follows the same
    general principles.  While the sorted portion is not a part of the
    array, but instead is a separate data structure, it does hold an
    initial part of the array in sorted order, and successive elements
    from the unsorted portion are inserted into it.
<p>
Because insertions into an AVL tree containing <i>k</i> elements can
      be done in <i>O</i>(lg <i>k</i>) time in the worst case, and
      because an inorder traversal can be done in <i>O</i>(<i>k</i>)
      time, it follows that tree sort runs in <i>O</i>(<i>n</i> lg
      <i>n</i>) time in the worst case, where <i>n</i> is the number
      of elements in the array.  However, because maintaining an AVL
      tree requires more overhead than maintaining a binary heap, heap
      sort tends to give better performance in practice.  For this
      reason, tree sort is rarely used.
</p>

<p></p>
<div class="NAVBAR">
<a href="/~rhowell/DataStructures/redirect/merge-sorts">Next: Merge
      Sorts 
      </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
      href="/~rhowell/DataStructures/redirect/select-sorts">Previous:
      Select Sorts
      </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
      href="/~rhowell/DataStructures/redirect/sorting">Up: Sorting
      </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
      href="/~rhowell/DataStructures/">Top</a>
</div>

<p>
<small>
<!-- hhmts start -->
Last modified: Fri May  1 11:17:45 CDT 2015
<!-- hhmts end -->
</small>
</p>
<small>
    <i> &copy; Copyright 2014, 2015, <a
    href="/~rhowell/">Rod Howell</a>. All
    rights reserved.</i> 
</small>

<p></p>

<table border=0 cellpadding=2 summary="This table is used for layout
purposes only.">
<tr>
<td align=center>
    <a href="http://validator.w3.org/check/referer"><img border="0"
        src="/~rhowell/valid-html401.gif"
        alt="Valid HTML 4.01!" height="31" width="88"></a>
</td>
<tr>
<td>
<a href="http://jigsaw.w3.org/css-validator/check/referer">
             <img border="0"
                  src="/~howell/vcss.gif" 
                  alt="Valid CSS!" height="31" width="88">
            </a>
</td>
</table>

</body>

</html>
